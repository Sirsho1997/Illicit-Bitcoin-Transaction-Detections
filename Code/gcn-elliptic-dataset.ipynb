{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Importing essential python libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nimport sys\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:38:35.021800Z","iopub.execute_input":"2022-06-24T13:38:35.022063Z","iopub.status.idle":"2022-06-24T13:38:41.164730Z","shell.execute_reply.started":"2022-06-24T13:38:35.022037Z","shell.execute_reply":"2022-06-24T13:38:41.163845Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Writing a function to extract the graph data from the dataset","metadata":{}},{"cell_type":"code","source":"#Function to load the dataset and create the adj matrix representation of the graph\n\ndef load_data(data_dir, start_ts, end_ts):\n    \n\tclasses_csv = 'elliptic_txs_classes.csv'\n\tedgelist_csv = 'elliptic_txs_edgelist.csv'\n\tfeatures_csv = 'elliptic_txs_features.csv'\n    \n    #Reading the .csv files\n\tclasses = pd.read_csv(os.path.join(data_dir, classes_csv), index_col = 'txId') # labels for the transactions i.e. 'unknown', '1', '2'\n\tedgelist = pd.read_csv(os.path.join(data_dir, edgelist_csv), index_col = 'txId1') # directed edges between transactions\n\tfeatures = pd.read_csv(os.path.join(data_dir, features_csv), header = None, index_col = 0) # features of the transactions\n\t\n\tnum_features = features.shape[1]\n\tnum_tx = features.shape[0]\t\n\ttotal_tx = list(classes.index)\n\n\t# Picking only the 'licit' and 'illicit' transaction nodes and not the 'unknown'\n\tlabelled_classes = classes[classes['class'] != 'unknown']\n\tlabelled_tx = list(labelled_classes.index)\n\n\t# to calculate a list of adjacency matrices for the different timesteps\n\n\tadj_mats = []\n\tfeatures_labelled_ts = []\n\tclasses_ts = []\n    \n    #Total number of timestamp used by the paper\n\tnum_ts = 49 \n\n\tfor ts in range(start_ts, end_ts):\n\t    features_ts = features[features[1] == ts+1]\n\t    tx_ts = list(features_ts.index)\n\t    \n\t    labelled_tx_ts = [tx for tx in tx_ts if tx in set(labelled_tx)]\n\t    \n\t    # Creating adjacency matrix for all the transactions\n\t    adj_mat = pd.DataFrame(np.zeros((num_tx, num_tx)), index = total_tx, columns = total_tx)\n\t    \n\t    edgelist_labelled_ts = edgelist.loc[edgelist.index.intersection(labelled_tx_ts).unique()]\n\t    for i in range(edgelist_labelled_ts.shape[0]):\n\t        adj_mat.loc[edgelist_labelled_ts.index[i], edgelist_labelled_ts.iloc[i]['txId2']] = 1\n\t    \n\t    adj_mat_ts = adj_mat.loc[labelled_tx_ts, labelled_tx_ts]\n\t    features_l_ts = features.loc[labelled_tx_ts]\n\t    \n\t    adj_mats.append(adj_mat_ts)\n\t    features_labelled_ts.append(features_l_ts)\n\t    classes_ts.append(classes.loc[labelled_tx_ts])\n\n\treturn adj_mats, features_labelled_ts, classes_ts","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:38:46.915683Z","iopub.execute_input":"2022-06-24T13:38:46.916010Z","iopub.status.idle":"2022-06-24T13:38:46.931375Z","shell.execute_reply.started":"2022-06-24T13:38:46.915978Z","shell.execute_reply":"2022-06-24T13:38:46.930382Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Adding the dataset to the notebook\ndir = \"../input/elliptic-data-set/elliptic_bitcoin_dataset\"\ndataSet = load_data(dir, 0, 34)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:38:52.764191Z","iopub.execute_input":"2022-06-24T13:38:52.764527Z","iopub.status.idle":"2022-06-24T13:45:07.893023Z","shell.execute_reply.started":"2022-06-24T13:38:52.764496Z","shell.execute_reply":"2022-06-24T13:45:07.892237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Defining the first layer of the GNN\n#Default properties: skip=False, activation= 'relu'\nclass GraphConv(nn.Module):\n    def __init__(self, in_features, out_features, activation  = 'relu', skip = False, skip_in_features = None):\n    #def __init__(self, in_features, out_features, activation  = 'relu', skip = True, skip_in_features = 1):\n        super(GraphConv, self).__init__()\n        self.W = torch.nn.Parameter(torch.DoubleTensor(in_features, out_features))\n        nn.init.xavier_uniform_(self.W)\n        \n        self.set_act = False\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n            self.set_act = True\n        elif activation == 'softmax':\n            self.activation = nn.Softmax(dim = 1)\n            self.set_act = True\n        else:\n            self.set_act = False\n            raise ValueError(\"activations supported are 'relu' and 'softmax'\")\n            \n        self.skip = skip\n        if self.skip:\n            if skip_in_features == None:\n                raise ValueError(\"pass input feature size of the skip connection\")\n            self.W_skip = torch.nn.Parameter(torch.DoubleTensor(skip_in_features, out_features)) \n            nn.init.xavier_uniform_(self.W)\n        \n    def forward(self, A, H_in, H_skip_in = None):\n        # A must be an n x n matrix as it is an adjacency matrix\n        # H is the input of the node embeddings, shape will n x in_features\n        self.A = A\n        self.H_in = H_in\n        A_ = torch.add(self.A, torch.eye(self.A.shape[0]).double())\n        D_ = torch.diag(A_.sum(1))\n        # since D_ is a diagonal matrix, \n        # its root will be the roots of the diagonal elements on the principle diagonal\n        # since A is an adjacency matrix, we are only dealing with positive values \n        # all roots will be real\n        D_root_inv = torch.inverse(torch.sqrt(D_))\n        A_norm = torch.mm(torch.mm(D_root_inv, A_), D_root_inv)\n        # shape of A_norm will be n x n\n        \n        H_out = torch.mm(torch.mm(A_norm, H_in), self.W)\n        # shape of H_out will be n x out_features\n        \n        if self.skip:\n            H_skip_out = torch.mm(H_skip_in, self.W_skip)\n            H_out = torch.add(H_out, H_skip_out)\n        \n        if self.set_act:\n            H_out = self.activation(H_out)\n            \n        return H_out","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:07:51.982249Z","iopub.execute_input":"2022-06-24T14:07:51.982586Z","iopub.status.idle":"2022-06-24T14:07:51.997652Z","shell.execute_reply.started":"2022-06-24T14:07:51.982553Z","shell.execute_reply":"2022-06-24T14:07:51.996713Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Defining the second layer of the GNN\n\nclass GCN_2layer(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features, skip = False):\n        super(GCN_2layer, self).__init__()\n        self.skip = skip\n        \n        self.gcl1 = GraphConv(in_features, hidden_features)\n        \n        if self.skip:\n            self.gcl_skip = GraphConv(hidden_features, out_features, activation = 'softmax', skip = self.skip,\n                                  skip_in_features = in_features)\n        else:\n            self.gcl2 = GraphConv(hidden_features, out_features, activation = 'softmax')\n        \n    def forward(self, A, X):\n        out = self.gcl1(A, X)\n        if self.skip:\n            out = self.gcl_skip(A, out, X)\n        else:\n            out = self.gcl2(A, out)\n            \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:08:58.312853Z","iopub.execute_input":"2022-06-24T14:08:58.313163Z","iopub.status.idle":"2022-06-24T14:08:58.321300Z","shell.execute_reply.started":"2022-06-24T14:08:58.313133Z","shell.execute_reply":"2022-06-24T14:08:58.320473Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Using 0 - illicit, 1 - licit\nSkip=False","metadata":{}},{"cell_type":"code","source":"num_features = 166\nnum_classes = 2\nnum_ts = 49\nepochs = 15\nlr = 0.001\nmax_train_ts = 34\ntrain_ts = np.arange(max_train_ts)\n\nadj_mats, features_labelled_ts, classes_ts = dataSet\n\n# 0 - illicit, 1 - licit\nlabels_ts = []\nfor c in classes_ts:\n    labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n\ngcn = GCN_2layer(num_features, 100, num_classes)\ntrain_loss = nn.CrossEntropyLoss(weight = torch.DoubleTensor([0.7, 0.3]))\noptimizer = torch.optim.Adam(gcn.parameters(), lr = lr)\n\n# Training\n\nfor ts in train_ts:\n    A = torch.tensor(adj_mats[ts].values)\n    X = torch.tensor(features_labelled_ts[ts].values)\n    L = torch.tensor(labels_ts[ts], dtype = torch.long)\n    for ep in range(epochs):\n        t_start = time.time()\n        \n        gcn.train()\n        optimizer.zero_grad()\n        out = gcn(A, X)\n\n        loss = train_loss(out, L)\n        train_pred = out.max(1)[1].type_as(L)\n        acc = (train_pred.eq(L).double().sum())/L.shape[0]\n\n        loss.backward()\n        optimizer.step()\n\n        sys.stdout.write(\"\\r Epoch %d/%d Timestamp %d/%d training loss: %f training accuracy: %f Time: %s\"\n                         %(ep, epochs, ts, max_train_ts, loss, acc, time.time() - t_start)\n                        )\n\n#torch.save(gcn.state_dict(), str(os.path.join(\"./modelDir\", \"gcn_weights.pth\")))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:09:02.851170Z","iopub.execute_input":"2022-06-24T14:09:02.851595Z","iopub.status.idle":"2022-06-24T14:11:40.955698Z","shell.execute_reply.started":"2022-06-24T14:09:02.851550Z","shell.execute_reply":"2022-06-24T14:11:40.954578Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Using Node Embedding for nodes where 0 - illicit, 1 - licit ","metadata":{}},{"cell_type":"code","source":"adj_mats, features_labelled_ts, classes_ts = load_data(dir, 35, 49)\n\nmax_train_ts = 14\ntrain_ts = np.arange(max_train_ts)\n\n\n# 0 - illicit, 1 - licit\nlabels_ts = []\nfor c in classes_ts:\n    labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:11:51.541322Z","iopub.execute_input":"2022-06-24T14:11:51.541692Z","iopub.status.idle":"2022-06-24T14:14:44.855761Z","shell.execute_reply.started":"2022-06-24T14:11:51.541662Z","shell.execute_reply":"2022-06-24T14:14:44.854844Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nfor ts in train_ts:\n    \n    A = torch.tensor(adj_mats[ts].values)\n    X = torch.tensor(features_labelled_ts[ts].values)\n    L = torch.tensor(labels_ts[ts], dtype = torch.long)\n    L = L.numpy()\n    L = np.vstack(L)\n    \n    \n    \n    gcn.eval()\n    test_out = gcn(A, X)\n    feature_map = gcn.gcl1(A, X)\n    \n    X=X.numpy()\n    \n    test_out=test_out.detach().numpy()\n    feature_map=feature_map.detach().numpy()\n    #print(feature_map.shape)\n    f=np.column_stack((X, feature_map))\n    if ts ==0:\n        t=f\n        l=L\n    else:\n        t=np.vstack((t,f))\n        l=np.vstack((l,L))\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:16:05.547267Z","iopub.execute_input":"2022-06-24T14:16:05.547608Z","iopub.status.idle":"2022-06-24T14:16:16.527801Z","shell.execute_reply.started":"2022-06-24T14:16:05.547579Z","shell.execute_reply":"2022-06-24T14:16:16.526899Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(t, l, test_size=0.30,random_state=15)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:18:49.514414Z","iopub.execute_input":"2022-06-24T14:18:49.514774Z","iopub.status.idle":"2022-06-24T14:18:49.528945Z","shell.execute_reply.started":"2022-06-24T14:18:49.514744Z","shell.execute_reply":"2022-06-24T14:18:49.528216Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=50 ,max_features=50).fit(X_train,y_train.ravel())\ny_preds = model.predict(X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:18:51.275444Z","iopub.execute_input":"2022-06-24T14:18:51.275872Z","iopub.status.idle":"2022-06-24T14:18:59.923264Z","shell.execute_reply.started":"2022-06-24T14:18:51.275839Z","shell.execute_reply":"2022-06-24T14:18:59.922511Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Accuracy\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Random Forest Classifier with NE where 1 - illicit, 0 - licit\")\nprint(\"Accuracy: \",accuracy_score(y_test.ravel(), y_preds))\n\n#Macro and Micro F1\nfrom sklearn.metrics import f1_score\nprint(\"Macro F1 score \",f1_score(y_test.ravel(), y_preds, average='macro'))\nprint(\"Micro F1 score \",f1_score(y_test.ravel(), y_preds, average='micro'))\n\n#AUC\nfrom sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test.ravel(), y_preds)\nprint(\"AUC: \",metrics.auc(fpr, tpr))\n\n#Precison Recall Curve\n\nfrom sklearn.metrics import precision_recall_curve\n\n#precision, recall, thresholds = precision_recall_curve(X_test, proba)\n\nprec_test,rec_test,thresholds_test = precision_recall_curve(y_test,model.predict_proba(X_test)[::,1])\nauc_pr_test = metrics.auc(rec_test,prec_test)\n\nprint(\"Precison Recall Curve\",auc_pr_test)\n#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion matrix\")\nprint(confusion_matrix(y_test.ravel(), y_preds))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:19:02.830483Z","iopub.execute_input":"2022-06-24T14:19:02.830808Z","iopub.status.idle":"2022-06-24T14:19:02.888488Z","shell.execute_reply.started":"2022-06-24T14:19:02.830781Z","shell.execute_reply":"2022-06-24T14:19:02.887562Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Using 1 - illicit, 0 - licit","metadata":{}},{"cell_type":"code","source":"num_features = 166\nnum_classes = 2\nnum_ts = 49\nepochs = 15\nlr = 0.001\nmax_train_ts = 34\ntrain_ts = np.arange(max_train_ts)\n\nadj_mats, features_labelled_ts, classes_ts = dataSet\n\n# 1 - illicit, 0 - licit\nlabels_ts = []\nfor c in classes_ts:\n    labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n    \nfor l in range(len(labels_ts)):\n    for k in range(len(labels_ts[l])):\n        if labels_ts[l][k]==0:\n            labels_ts[l][k]=2\nfor l in range(len(labels_ts)):\n    for k in range(len(labels_ts[l])):\n        if labels_ts[l][k]==1:\n            labels_ts[l][k]=0\nfor l in range(len(labels_ts)):\n    for k in range(len(labels_ts[l])):\n        if labels_ts[l][k]==2:\n            labels_ts[l][k]=1\n\ngcn = GCN_2layer(num_features, 100, num_classes)\ntrain_loss = nn.CrossEntropyLoss(weight = torch.DoubleTensor([0.3, 0.7]))\noptimizer = torch.optim.Adam(gcn.parameters(), lr = lr)\n\n# Training\n\nfor ts in train_ts:\n    A = torch.tensor(adj_mats[ts].values)\n    X = torch.tensor(features_labelled_ts[ts].values)\n    L = torch.tensor(labels_ts[ts], dtype = torch.long)\n    for ep in range(epochs):\n        t_start = time.time()\n        \n        gcn.train()\n        optimizer.zero_grad()\n        out = gcn(A, X)\n\n        loss = train_loss(out, L)\n        train_pred = out.max(1)[1].type_as(L)\n        acc = (train_pred.eq(L).double().sum())/L.shape[0]\n\n        loss.backward()\n        optimizer.step()\n\n        sys.stdout.write(\"\\r Epoch %d/%d Timestamp %d/%d training loss: %f training accuracy: %f Time: %s\"\n                         %(ep, epochs, ts, max_train_ts, loss, acc, time.time() - t_start)\n                        )\n\n#torch.save(gcn.state_dict(), str(os.path.join(\"./modelDir\", \"gcn_weights.pth\")))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:19:08.783894Z","iopub.execute_input":"2022-06-24T14:19:08.784212Z","iopub.status.idle":"2022-06-24T14:21:46.898400Z","shell.execute_reply.started":"2022-06-24T14:19:08.784184Z","shell.execute_reply":"2022-06-24T14:21:46.897545Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Using Node Embedding for nodes where 1 - illicit, 0 - licit ","metadata":{}},{"cell_type":"code","source":"adj_mats, features_labelled_ts, classes_ts = load_data(dir, 35, 49)\n\nmax_train_ts = 14\ntrain_ts = np.arange(max_train_ts)\n\n\n# 1 - illicit, 0 - licit\nlabels_ts = []\nfor c in classes_ts:\n    labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n    \nfor l in range(len(labels_ts)):\n    for k in range(len(labels_ts[l])):\n        if labels_ts[l][k]==0:\n            labels_ts[l][k]=2\nfor l in range(len(labels_ts)):\n    for k in range(len(labels_ts[l])):\n        if labels_ts[l][k]==1:\n            labels_ts[l][k]=0\nfor l in range(len(labels_ts)):\n    for k in range(len(labels_ts[l])):\n        if labels_ts[l][k]==2:\n            labels_ts[l][k]=1\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:23:00.269288Z","iopub.execute_input":"2022-06-24T14:23:00.269648Z","iopub.status.idle":"2022-06-24T14:25:51.258206Z","shell.execute_reply.started":"2022-06-24T14:23:00.269618Z","shell.execute_reply":"2022-06-24T14:25:51.257217Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\nfor ts in train_ts:\n    \n    A = torch.tensor(adj_mats[ts].values)\n    X = torch.tensor(features_labelled_ts[ts].values)\n    L = torch.tensor(labels_ts[ts], dtype = torch.long)\n    L = L.numpy()\n    L = np.vstack(L)\n    \n    \n    \n    gcn.eval()\n    test_out = gcn(A, X)\n    feature_map = gcn.gcl1(A, X)\n    X=X.numpy()\n    \n    test_out=test_out.detach().numpy()\n    feature_map=feature_map.detach().numpy()\n    #print(feature_map.shape)\n    \n    f=np.column_stack((X, feature_map))\n    if ts ==0:\n        t=f\n        l=L\n    else:\n        t=np.vstack((t,f))\n        l=np.vstack((l,L))\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:26:01.610237Z","iopub.execute_input":"2022-06-24T14:26:01.610592Z","iopub.status.idle":"2022-06-24T14:26:12.320970Z","shell.execute_reply.started":"2022-06-24T14:26:01.610557Z","shell.execute_reply":"2022-06-24T14:26:12.320038Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(t, l, test_size=0.30,random_state=15)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:27:00.785203Z","iopub.execute_input":"2022-06-24T14:27:00.785567Z","iopub.status.idle":"2022-06-24T14:27:00.802093Z","shell.execute_reply.started":"2022-06-24T14:27:00.785531Z","shell.execute_reply":"2022-06-24T14:27:00.801364Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=50 ,max_features=50).fit(X_train,y_train.ravel())\ny_preds = model.predict(X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:27:03.234693Z","iopub.execute_input":"2022-06-24T14:27:03.235012Z","iopub.status.idle":"2022-06-24T14:27:11.558736Z","shell.execute_reply.started":"2022-06-24T14:27:03.234983Z","shell.execute_reply":"2022-06-24T14:27:11.557861Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#Accuracy\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Random Forest Classifier with NE where 1 - illicit, 0 - licit\")\nprint(\"Accuracy: \",accuracy_score(y_test.ravel(), y_preds))\n\n#Macro and Micro F1\nfrom sklearn.metrics import f1_score\nprint(\"Macro F1 score \",f1_score(y_test.ravel(), y_preds, average='macro'))\nprint(\"Micro F1 score \",f1_score(y_test.ravel(), y_preds, average='micro'))\n\n#AUC\nfrom sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test.ravel(), y_preds)\nprint(\"AUC: \",metrics.auc(fpr, tpr))\n\n#Precison Recall Curve\n\nfrom sklearn.metrics import precision_recall_curve\nprint(\"Precison Recall Curve\")\n#precision, recall, thresholds = precision_recall_curve(X_test, proba)\n\nprec_test,rec_test,thresholds_test = precision_recall_curve(y_test,model.predict_proba(X_test)[::,1])\nauc_pr_test = metrics.auc(rec_test,prec_test)\n\nprint(auc_pr_test)\n#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion matrix\")\nprint(confusion_matrix(y_test.ravel(), y_preds))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T14:27:18.870699Z","iopub.execute_input":"2022-06-24T14:27:18.871004Z","iopub.status.idle":"2022-06-24T14:27:18.927646Z","shell.execute_reply.started":"2022-06-24T14:27:18.870978Z","shell.execute_reply":"2022-06-24T14:27:18.926794Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}